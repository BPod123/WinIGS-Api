{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e74e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the operating system operations e.g., creating a folder.\n",
    "import os\n",
    "\n",
    "# Tensorflow and Keras are two packages for creating neural network models.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f36f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NN layers and other componenets.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for plotting data and creating different charts.\n",
    "import numpy as np # for math and arrays\n",
    "import pandas as pd # data from for the data.\n",
    "import seaborn as sns # for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399cc855",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(13) # to make sure the experiment is reproducible.\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4857ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds = pd.read_csv('diabetes.csv')\n",
    "all_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rows/examples and columns in the dataset: {all_ds.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c9249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of information on the dataset.\n",
    "all_ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da63070",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Display NA values in each columns: \")\n",
    "all_ds.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dee863",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Display NA values in each row: \")\n",
    "all_ds.isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122297ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Display NULL values in each columns: \")\n",
    "all_ds.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Display NULL values in each row: \")\n",
    "all_ds.isnull().sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ddab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with NA values.\n",
    "all_ds = all_ds.dropna()\n",
    "\n",
    "# You can also use the following line to fill the NA with Zeros.\n",
    "# all_ds = all_ds.fillna(0)\n",
    "# all_ds = all_ds.fillna(mean_of_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038dafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the data set and keep last n rows of the dataset.  E.g., you want to save the last 20 rows from the dataset into a new dataset.\n",
    "n = 20\n",
    "temp_ds = all_ds[:n]\n",
    "temp_ds.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ds[10:]\n",
    "# all_ds.sample(frac=1) # this will shuffle all of the dataset and randomly re-organize the rows.\n",
    "\n",
    "all_ds = all_ds.sample(frac=1) # This will randomly shuffle the rows to make sure the data is not sorted. (if the data is sorted then we may end up with test dataset from one or two classes only)\n",
    "# all_ds_90pct = all_ds.sample(frac=0.9) # randomly sample the dataset and keep 90% of the rows.\n",
    "# all_ds_90pct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcdb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis is an alternative method to split the data and replace train_test_split.\n",
    "# train_dataset = all_ds_90pct.sample(frac=0.6) # This will select 80\n",
    "# test_dataset = all_ds_90pct.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 60% train and 40% test (later will divide the test to test and validate.)\n",
    "train_dataset, temp_test_dataset =  train_test_split(all_ds, test_size=0.4)\n",
    "\n",
    "print( train_dataset.shape )\n",
    "print( temp_test_dataset.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test_dataset dataframe to 50% test and 50% validation. [this will divide the dataset into 60% train, 20% validate, and 20% test]\n",
    "test_dataset, valid_dataset =  train_test_split(temp_test_dataset, test_size=0.5)\n",
    "print( test_dataset.shape )\n",
    "print( valid_dataset.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Display the datatype of the test_dataset: {type(test_dataset)}\")\n",
    "print(f\" Trai dataset       : {train_dataset.shape}\")\n",
    "print(f\" Test dataset       : {test_dataset.shape}\")\n",
    "print(f\" Validation dataset : {valid_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6218939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between each two variables to spot anything incorrect.\n",
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop(\"class\")\n",
    "sns.pairplot(train_stats[train_stats.columns], diag_kind=\"kde\") # or diag_kind='reg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f7d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics on the train dataset to make sure it is in a good shape. (you may display the same stat for test and validate)\n",
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop(\"class\")\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b494237",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_dataset.pop('class')\n",
    "test_labels = test_dataset.pop('class')\n",
    "valid_labels = valid_dataset.pop('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a sample of the data after normalized\n",
    "normed_train_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb7300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decalred a function for creating a model.\n",
    "def build_model2_three_hidden_layers():\n",
    "    # Keras model object created from Sequential class. This will be the container that contains all layers.\n",
    "    model = Sequential()\n",
    "\n",
    "    # The model so far is empty. It can be constructed by adding layers and compilation.\n",
    "    # This Keras model with multiple hidden layers.\n",
    "    \n",
    "    # Input Layer with 32 Neurons\n",
    "    model.add(Dense(32, input_shape = (normed_train_data.shape[1],)))    # Input layer => input_shape must be explicitly designated\n",
    "#     model.add(Activation('relu')) # relu or sigmoid.\n",
    "    \n",
    "    model.add(Dense(32,Activation('relu')))                         # Hidden layer 1 => only output dimension should be designated (output dimension = # of Neurons = 32)\n",
    "    \n",
    "    \n",
    "    model.add(Dense(64, Activation('relu')))                         # Hidden layer 2 => only output dimension should be designated (output dimension = # of Neurons = 64)\n",
    "    \n",
    "    \n",
    "    model.add(Dense(128, Activation('relu')))                         # Hidden layer 3 => only output dimension should be designated (output dimension = # of Neurons = 128)\n",
    "\n",
    "    \n",
    "    model.add(Dense(1))                          # Output layer => output dimension = 1 since it is a regression problem\n",
    "    \n",
    "    # Activation: sigmoid, softmax, tanh, relu, LeakyReLU. \n",
    "    #Optimizer: SGD, Adam, RMSProp, etc. # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "    learning_rate = 0.001\n",
    "    optimizer = optimizers.SGD(learning_rate)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy']) \n",
    "    return model\n",
    "\n",
    "model2 = build_model2_three_hidden_layers()\n",
    "print('Here is a summary of this model: ')\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f1310",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# We decalred a function for creating a model.\n",
    "def build_model1_two_hidden_layers():\n",
    "    # Keras model object created from Sequential class. This will be the container that contains all layers.\n",
    "    model = Sequential()\n",
    "\n",
    "    # The model so far is empty. It can be constructed by adding layers and compilation.\n",
    "    # This Keras model with multiple hidden layers.\n",
    "    \n",
    "    # Input Layer with 10 Neurons\n",
    "    model.add(Dense(32, input_shape = (normed_train_data.shape[1],)))    # Input layer => input_shape must be explicitly designated\n",
    "#     model.add(Activation('relu')) # relu or sigmoid.\n",
    "    \n",
    "#     model.add(Dense(128,Activation('relu')))                         # Hidden layer 1 => only output dimension should be designated (output dimension = # of Neurons = 50)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Dense(1))                          # Output layer => output dimension = 1 since it is a regression problem\n",
    "    \n",
    "    # Activation: sigmoid, softmax, tanh, relu, LeakyReLU. \n",
    "    #Optimizer: SGD, Adam, RMSProp, etc. # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "    learning_rate = 0.0001\n",
    "    optimizer = optimizers.SGD(learning_rate)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy']) # for regression problems, mean squared error (MSE) is often employed\n",
    "    return model\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "batch_size = 16 # 6 iteration\n",
    "\n",
    "model = build_model1_two_hidden_layers()\n",
    "print('Here is a summary of this model: ')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "with tf.device('/CPU:0'): # it can be with '/CPU:0'\n",
    "# with tf.device('/GPU:0'): # comment the previous line and uncomment this line to train with a GPU, if available.\n",
    "    history = model.fit(\n",
    "        normed_train_data, \n",
    "        train_labels,\n",
    "        batch_size = batch_size,\n",
    "        epochs=EPOCHS, \n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "        steps_per_epoch = int(normed_train_data.shape[0] / batch_size) ,\n",
    "        validation_data = (normed_valid_dataset, valid_labels),   \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f120cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary of the results after each epoch: ')\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f016d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Cross-Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Cross-Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Split: ')\n",
    "loss, accuracy = model.evaluate(normed_train_data, train_labels, verbose=1)\n",
    "\n",
    "print(\"Accuracy   : {:5.2f} \".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2fa34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluation Split: ')\n",
    "loss, accuracy =  model.evaluate(normed_valid_dataset, valid_labels, verbose=2)\n",
    "\n",
    "print(\"Accuracy   : {:5.2f} \".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcad1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Split: ')\n",
    "loss, accuracy =  model.evaluate(normed_test_data, test_labels, verbose=2)\n",
    "\n",
    "print(\"Accuracy   : {:5.2f} \".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt     \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "ax= plt.subplot()\n",
    "predict_results = model.predict(normed_test_data)\n",
    "\n",
    "predict_results = (predict_results > 0.5)\n",
    "\n",
    "cm = confusion_matrix(test_labels, predict_results)\n",
    "\n",
    "sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['Positive', 'Negative'])\n",
    "ax.yaxis.set_ticklabels(['Positive', 'Negative'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
